{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Computing similarity between terms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPPJkkxbc3eN"
      },
      "source": [
        "#  1. Set up Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRushJxTdUch"
      },
      "source": [
        "## Required Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EtckRDHE4VxV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize, RegexpTokenizer\n",
        "from nltk import pos_tag\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from enum import Enum\n",
        "\n",
        "from random import shuffle\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "from time import time\n",
        "\n",
        "from datetime import timedelta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50q1tB2-dXBq"
      },
      "source": [
        "## Required NLTK Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNRx_BBwn5lf",
        "outputId": "01220cdc-cb28-4810-f289-4e47af7ea45f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\klouc\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\klouc\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\klouc\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j_wYGNRdgBQ"
      },
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "O9YH36aJ7WVm"
      },
      "outputs": [],
      "source": [
        "CWD = \"./data\"\n",
        "TRAINING_DATA = 'Training-dataset.csv'\n",
        "VALIDATION_DATA = 'Validation-dataset-terms.csv'\n",
        "TEST_DATA = 'Test-dataset-terms.csv'\n",
        "\n",
        "PLOT_SYNOPSIS = 'plot_synopsis'\n",
        "PREDICTION = 'prediction'\n",
        "\n",
        "Method = Enum('Method', ['PPMI', 'WORD2VEC'])\n",
        "Data = Enum('Type', ['VALIDATION', 'TEST'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPCOuol5fCVn"
      },
      "source": [
        "# 2. Load in Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfGokTjYZPuO"
      },
      "source": [
        "## Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 973
        },
        "id": "hdFxicpg5KS3",
        "outputId": "ef3541c0-3ccc-456a-9abd-e6798497f606"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training movie data count: 8257\n",
            "Training movie data count after NaN check: 8257\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>title</th>\n",
              "      <th>plot_synopsis</th>\n",
              "      <th>comedy</th>\n",
              "      <th>cult</th>\n",
              "      <th>flashback</th>\n",
              "      <th>historical</th>\n",
              "      <th>murder</th>\n",
              "      <th>revenge</th>\n",
              "      <th>romantic</th>\n",
              "      <th>scifi</th>\n",
              "      <th>violence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8f5203de-b2f8-4c0c-b0c1-835ba92422e9</td>\n",
              "      <td>Si wang ta</td>\n",
              "      <td>After a recent amount of challenges, Billy Lo ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6416fe15-6f8a-41d4-8a78-3e8f120781c7</td>\n",
              "      <td>Shattered Vengeance</td>\n",
              "      <td>In the crime-ridden city of Tremont, renowned ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4979fe9a-0518-41cc-b85f-f364c91053ca</td>\n",
              "      <td>L'esorciccio</td>\n",
              "      <td>Lankester Merrin is a veteran Catholic priest ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>b672850b-a1d9-44ed-9cff-025ee8b61e6f</td>\n",
              "      <td>Serendipity Through Seasons</td>\n",
              "      <td>\"Serendipity Through Seasons\" is a heartwarmin...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>b4d8e8cc-a53e-48f8-be6a-6432b928a56d</td>\n",
              "      <td>The Liability</td>\n",
              "      <td>Young and naive 19-year-old slacker, Adam (Jac...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2fbcdf4a-1c10-4958-a175-985d226f5906</td>\n",
              "      <td>Savage Vengance</td>\n",
              "      <td>Katie Carter (Dallender) is an aspiring model ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>60298c01-41d0-4e12-a203-b5a1feb78943</td>\n",
              "      <td>The Snake Pit</td>\n",
              "      <td>Virginia Cunningham (Olivia de Havilland) is a...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>084f6cc3-e4e2-4f1a-bcbb-b26dbdd2762f</td>\n",
              "      <td>Shadows of Betrayal</td>\n",
              "      <td>In the dark and gritty underbelly of a sprawli...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>a198118a-564e-42f8-8df2-0cbec828aa2f</td>\n",
              "      <td>Kakushi ken oni no tsume</td>\n",
              "      <td>The film takes place in Japan in the 1860s, a ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>541bbc68-5628-43a3-9f83-49c7900c2e57</td>\n",
              "      <td>Intolerable Cruelty</td>\n",
              "      <td>Donovan Donaly (Geoffrey Rush) a TV soap opera...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     ID                        title  \\\n",
              "0  8f5203de-b2f8-4c0c-b0c1-835ba92422e9                   Si wang ta   \n",
              "1  6416fe15-6f8a-41d4-8a78-3e8f120781c7          Shattered Vengeance   \n",
              "2  4979fe9a-0518-41cc-b85f-f364c91053ca                 L'esorciccio   \n",
              "3  b672850b-a1d9-44ed-9cff-025ee8b61e6f  Serendipity Through Seasons   \n",
              "4  b4d8e8cc-a53e-48f8-be6a-6432b928a56d                The Liability   \n",
              "5  2fbcdf4a-1c10-4958-a175-985d226f5906              Savage Vengance   \n",
              "6  60298c01-41d0-4e12-a203-b5a1feb78943                The Snake Pit   \n",
              "7  084f6cc3-e4e2-4f1a-bcbb-b26dbdd2762f          Shadows of Betrayal   \n",
              "8  a198118a-564e-42f8-8df2-0cbec828aa2f     Kakushi ken oni no tsume   \n",
              "9  541bbc68-5628-43a3-9f83-49c7900c2e57          Intolerable Cruelty   \n",
              "\n",
              "                                       plot_synopsis  comedy  cult  flashback  \\\n",
              "0  After a recent amount of challenges, Billy Lo ...       0     0          0   \n",
              "1  In the crime-ridden city of Tremont, renowned ...       0     0          0   \n",
              "2  Lankester Merrin is a veteran Catholic priest ...       0     1          0   \n",
              "3  \"Serendipity Through Seasons\" is a heartwarmin...       0     0          0   \n",
              "4  Young and naive 19-year-old slacker, Adam (Jac...       0     0          1   \n",
              "5  Katie Carter (Dallender) is an aspiring model ...       0     0          0   \n",
              "6  Virginia Cunningham (Olivia de Havilland) is a...       0     0          1   \n",
              "7  In the dark and gritty underbelly of a sprawli...       0     0          0   \n",
              "8  The film takes place in Japan in the 1860s, a ...       0     0          0   \n",
              "9  Donovan Donaly (Geoffrey Rush) a TV soap opera...       1     0          0   \n",
              "\n",
              "   historical  murder  revenge  romantic  scifi  violence  \n",
              "0           0       1        1         0      0         1  \n",
              "1           0       1        1         1      0         1  \n",
              "2           0       0        0         0      0         0  \n",
              "3           0       0        0         1      0         0  \n",
              "4           0       0        0         0      0         0  \n",
              "5           0       1        0         0      0         0  \n",
              "6           0       0        0         0      0         0  \n",
              "7           0       1        0         0      0         1  \n",
              "8           0       1        1         0      0         0  \n",
              "9           0       0        1         1      0         0  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data = pd.read_csv(f\"{CWD}/{TRAINING_DATA}\")\n",
        "print(f'Training movie data count: {len(train_data.values)}')\n",
        "\n",
        "train_data.dropna(inplace=True)\n",
        "train_data.reset_index(inplace=True, drop=True)\n",
        "print(f'Training movie data count after NaN check: {len(train_data.values)}')\n",
        "\n",
        "train_data.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmCHs7UafJ3O"
      },
      "source": [
        "## Validation Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "joEh-bFc9JXw",
        "outputId": "aa3bb7bb-11b3-4003-99dc-07deb673957a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation movie data count: 150\n",
            "Validation movie data after NaN check count: 150\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>absorb</td>\n",
              "      <td>learn</td>\n",
              "      <td>5.48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>absorb</td>\n",
              "      <td>withdraw</td>\n",
              "      <td>2.97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>achieve</td>\n",
              "      <td>accomplish</td>\n",
              "      <td>8.57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>achieve</td>\n",
              "      <td>try</td>\n",
              "      <td>4.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>acquire</td>\n",
              "      <td>get</td>\n",
              "      <td>8.82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>7</td>\n",
              "      <td>acquire</td>\n",
              "      <td>obtain</td>\n",
              "      <td>8.57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>8</td>\n",
              "      <td>acquire</td>\n",
              "      <td>find</td>\n",
              "      <td>6.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>11</td>\n",
              "      <td>apple</td>\n",
              "      <td>sauce</td>\n",
              "      <td>1.43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>12</td>\n",
              "      <td>apple</td>\n",
              "      <td>lemon</td>\n",
              "      <td>4.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>13</td>\n",
              "      <td>apple</td>\n",
              "      <td>sunshine</td>\n",
              "      <td>0.58</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    0        1           2     3\n",
              "0   1   absorb       learn  5.48\n",
              "1   2   absorb    withdraw  2.97\n",
              "2   3  achieve  accomplish  8.57\n",
              "3   4  achieve         try  4.42\n",
              "4   6  acquire         get  8.82\n",
              "5   7  acquire      obtain  8.57\n",
              "6   8  acquire        find  6.38\n",
              "7  11    apple       sauce  1.43\n",
              "8  12    apple       lemon  4.05\n",
              "9  13    apple    sunshine  0.58"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_data = pd.read_csv(f\"{CWD}/{VALIDATION_DATA}\", header=None)\n",
        "print(f'Validation movie data count: {len(val_data.values)}')\n",
        "\n",
        "val_data.dropna(inplace=True)\n",
        "val_data.reset_index(inplace=True, drop=True)\n",
        "print(f'Validation movie data after NaN check count: {len(val_data.values)}')\n",
        "\n",
        "val_data.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aEStM7rfNDa"
      },
      "source": [
        "## Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "e-HiVfz7fVdV",
        "outputId": "96654676-f01e-45a3-b3aa-7459d29e4225"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test movie data count: 102\n",
            "Test movie data after NaN check count: 102\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>816</td>\n",
              "      <td>accept</td>\n",
              "      <td>acknowledge</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>957</td>\n",
              "      <td>accept</td>\n",
              "      <td>recommend</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>809</td>\n",
              "      <td>agree</td>\n",
              "      <td>argue</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>911</td>\n",
              "      <td>agree</td>\n",
              "      <td>please</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>242</td>\n",
              "      <td>alcohol</td>\n",
              "      <td>cocktail</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>697</td>\n",
              "      <td>alcohol</td>\n",
              "      <td>wine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2066</td>\n",
              "      <td>announcement</td>\n",
              "      <td>news</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2164</td>\n",
              "      <td>announcement</td>\n",
              "      <td>effort</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>14</td>\n",
              "      <td>bad</td>\n",
              "      <td>terrible</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>51</td>\n",
              "      <td>bad</td>\n",
              "      <td>great</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      0             1            2\n",
              "0   816        accept  acknowledge\n",
              "1   957        accept    recommend\n",
              "2   809         agree        argue\n",
              "3   911         agree       please\n",
              "4   242       alcohol     cocktail\n",
              "5   697       alcohol         wine\n",
              "6  2066  announcement         news\n",
              "7  2164  announcement       effort\n",
              "8    14           bad     terrible\n",
              "9    51           bad        great"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_data = pd.read_csv(f\"{CWD}/{TEST_DATA}\", header=None)\n",
        "print(f'Test movie data count: {len(test_data.values)}')\n",
        "\n",
        "test_data.dropna(inplace=True)\n",
        "test_data.reset_index(inplace=True, drop=True)\n",
        "print(f'Test movie data after NaN check count: {len(test_data.values)}')\n",
        "\n",
        "test_data.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ye0u-pQefTxx"
      },
      "source": [
        "## Extract Plot Synopses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgQ9gN2DfSno",
        "outputId": "32623ac5-09f2-4dcc-d964-c950b12c574e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    After a recent amount of challenges, Billy Lo ...\n",
              "1    In the crime-ridden city of Tremont, renowned ...\n",
              "2    Lankester Merrin is a veteran Catholic priest ...\n",
              "3    \"Serendipity Through Seasons\" is a heartwarmin...\n",
              "4    Young and naive 19-year-old slacker, Adam (Jac...\n",
              "5    Katie Carter (Dallender) is an aspiring model ...\n",
              "6    Virginia Cunningham (Olivia de Havilland) is a...\n",
              "7    In the dark and gritty underbelly of a sprawli...\n",
              "8    The film takes place in Japan in the 1860s, a ...\n",
              "9    Donovan Donaly (Geoffrey Rush) a TV soap opera...\n",
              "Name: plot_synopsis, dtype: object"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "plot_synopses = train_data[PLOT_SYNOPSIS]\n",
        "plot_synopses.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3kH374Zdxji"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VaXFH6XhuQLw"
      },
      "outputs": [],
      "source": [
        "def get_cosine_similarity(first_term, second_term, method):\n",
        "  \"\"\"\n",
        "  Get the cosine similarity between two terms (can be multi-worded) by first\n",
        "  creating vector representations for them using one of the pre-defined methods\n",
        "  (PPMI or word2vec).\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  first_term : str\n",
        "      The first term, if multi-worded the different words are separated by\n",
        "      spaces.\n",
        "  second_term : str\n",
        "      The second term, if multi-worded the different words are separated by\n",
        "      spaces.\n",
        "  method : Method\n",
        "      The method with which to vectorize the terms, can only be word2vec or\n",
        "      PPMI.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  float\n",
        "      The cosine similarity between the two vectors that represent the two\n",
        "      terms.\n",
        "  \"\"\"\n",
        "  # Validate requested method\n",
        "  try:\n",
        "    if(method not in Method):\n",
        "      return None\n",
        "  except:\n",
        "    print(\"Please choose one of the valid methods (PPMI or WORD2VEC).\")\n",
        "\n",
        "  # Get vector representations of terms using specified method\n",
        "  if(method == Method.PPMI):\n",
        "    first_term_vector = ppmi_vectorizer.get_term_vector(first_term)\n",
        "    second_term_vector = ppmi_vectorizer.get_term_vector(second_term)\n",
        "  elif(method == Method.WORD2VEC):\n",
        "    first_term_vector = word2vec_vectorizer.get_term_vector(first_term)\n",
        "    second_term_vector = word2vec_vectorizer.get_term_vector(second_term)\n",
        "  else:\n",
        "    print(\"Please choose one of the valid methods (PPMI or WORD2VEC).\")\n",
        "    return None\n",
        "\n",
        "  # Reshape into correct matrix format for cosine_similarity function\n",
        "  first_term_vector = first_term_vector.reshape(1, -1)\n",
        "  second_term_vector = second_term_vector.reshape(1, -1)\n",
        "\n",
        "  # Get similarity score and extract value from (1, 1)-sized matrix\n",
        "  similarity_score = cosine_similarity(first_term_vector,\n",
        "                                      second_term_vector)\n",
        "\n",
        "  return similarity_score[0, 0]\n",
        "\n",
        "\n",
        "def get_comparison_with_csv(method, data):\n",
        "  \"\"\"\n",
        "  Compute the cosine similarities between all term pairs within a csv file.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  method : Method\n",
        "      The method with which to vectorize the terms, can only be word2vec or\n",
        "      PPMI.\n",
        "  data : Data\n",
        "      The type of data to use when looking for which csv to extract term pairs\n",
        "      from. Can only be from the validation or test data.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  pandas.DataFrame\n",
        "      A DataFrame object that contains all cosine similarities between each\n",
        "      term pair by creating the vectors that represent each associated term.\n",
        "  \"\"\"\n",
        "  # Validate requested method\n",
        "  try:\n",
        "    if(method not in Method):\n",
        "      return None\n",
        "  except:\n",
        "    print(\"Please choose one of the valid methods (PPMI or WORD2VEC).\")\n",
        "\n",
        "  # Validate requested data file\n",
        "  try:\n",
        "    if(data not in Data):\n",
        "      return None\n",
        "  except:\n",
        "    print(\"Please choose one of the valid datasets (VALIDATION or TEST).\")\n",
        "\n",
        "  # Set data to compute similarity for\n",
        "  comparison_data = val_data\n",
        "  if(data == Data.TEST):\n",
        "    comparison_data = test_data\n",
        "\n",
        "  start_time = time()\n",
        "  comparison_data[PREDICTION] = comparison_data.apply(lambda row:\n",
        "                                                        get_cosine_similarity(\n",
        "                                                            row[1],\n",
        "                                                            row[2],\n",
        "                                                            method),\n",
        "                                                        axis=1)\n",
        "  end_time = time()\n",
        "\n",
        "  # Calculate elapsed time\n",
        "  elapsed_time = str(timedelta(seconds=end_time - start_time))\n",
        "  print(f\"Time taken to compute similarities for given data: \" +\n",
        "  f\"{str(elapsed_time)[elapsed_time.find(':') + 1:]}\")\n",
        "\n",
        "  return comparison_data[[0, PREDICTION]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSZ2Q3O_fl6v"
      },
      "source": [
        "# 3. Implement Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OttCFmtQZ-FS"
      },
      "source": [
        "## METHOD A) Sparse Representation\n",
        "### PPMI (Bag of Words with Positive Pointwise Mutual Information)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0gROR5Ek07F"
      },
      "source": [
        "### Impement and initialise PPMI class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OtAKhBtX7OHQ"
      },
      "outputs": [],
      "source": [
        "class PPMIVectorizer():\n",
        "  \"\"\"\n",
        "  A custom class for PPMI that allows to dynamically generate sparse vector\n",
        "  representations for terms (including multi-word terms) by going through\n",
        "  a corpus with a specific context window length hyperparameter.\n",
        "\n",
        "  Attributes\n",
        "  ----------\n",
        "  term_term_count : defaultdict of str to int\n",
        "      A dictionary of tuple detailing the count of how often a ceratin term\n",
        "      appears in an another term's context depending on the context window\n",
        "      length.\n",
        "  term_sums : defaultdict of tuple of str and str to int\n",
        "      The occurrence sums for each term.\n",
        "  total_occurrence_sum : int\n",
        "      A sum of all term occurrences.\n",
        "  vocabulary : set of str\n",
        "      A set of unique words within the preprocessed corpus.\n",
        "  smoother : float\n",
        "      Small value to apply smoothing to vectors.\n",
        "\n",
        "  Methods\n",
        "  -------\n",
        "  preprocess_corpus(corpus)\n",
        "      Preprocess a given corpus and return a list of tokenised sentences.\n",
        "  count_terms_with_context(corpus, context_window_len)\n",
        "      Populate PPMI's signature attributes using the context window length.\n",
        "  get_term_vector(term)\n",
        "      Dynamically compute the vector representation of a term (can be multi-\n",
        "      worded).\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    Initialize the PPMIVectorizer object.\n",
        "    \"\"\"\n",
        "    self.term_term_count = defaultdict(int)\n",
        "    self.term_sums = defaultdict(int)\n",
        "    self.total_occurrence_sum = 0\n",
        "    self.vocabulary = set()\n",
        "    self.smoother = 1e-8\n",
        "\n",
        "  def preprocess_corpus(self, corpus):\n",
        "    \"\"\"\n",
        "    Preprocess a given corpus and return a list of tokenised sentences.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    corpus : pandas.Series of str\n",
        "        Series of strings containing all documents which need to be\n",
        "        preprocessed.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list of list of str\n",
        "        A list of lists of strings that are the tokenised words from the\n",
        "        corpus.\n",
        "    \"\"\"\n",
        "    word_tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    preprocessed_sentences = []\n",
        "\n",
        "    print('Processing texts...')\n",
        "    for text in tqdm(corpus):\n",
        "      # Tokenize sentences and apply lower case\n",
        "      sents = sent_tokenize(text)\n",
        "      for sent in sents:\n",
        "        # Tokenise words\n",
        "        words = word_tokenizer.tokenize(sent)\n",
        "        # Tag POS and convert from penn to wordnet schema\n",
        "        words_tagged = pos_tag(words)\n",
        "        words_tagged_wn = [(word_tag_pair[0], 'a') if word_tag_pair[1][0].lower() == 'j' else (word_tag_pair[0], word_tag_pair[1][0].lower()) if word_tag_pair[1][0].lower() in ['n', 'r', 'v'] else (word_tag_pair[0], 'n') for word_tag_pair in words_tagged]\n",
        "        # Lemmatize each word\n",
        "        words = [lemmatizer.lemmatize(word_tag_pair[0], word_tag_pair[1]) for word_tag_pair in words_tagged_wn]\n",
        "        # If sentence too short no context available, skip\n",
        "        if(len(words)) < 2:\n",
        "          continue\n",
        "\n",
        "        preprocessed_sentences.append(words)\n",
        "    return preprocessed_sentences\n",
        "\n",
        "  def count_terms_with_context(self, corpus, context_window_len=1):\n",
        "    \"\"\"\n",
        "    Populate PPMI's signature attributes using the context window length.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    corpus : pandas.Series of str\n",
        "        Series of strings containing all documents.\n",
        "    context_window_len : int\n",
        "        The length of the context window which would indicate if a term falls\n",
        "        within another term's context.\n",
        "    \"\"\"\n",
        "    word_tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    preprocessed_sentences = self.preprocess_corpus(corpus)\n",
        "\n",
        "    for tokenised_sent in preprocessed_sentences:\n",
        "      # Get subsequent tokens within context window\n",
        "      for i in range(len(tokenised_sent)):\n",
        "          curr_word = tokenised_sent[i]\n",
        "          self.vocabulary.add(curr_word)\n",
        "          subsequent_tokens = tokenised_sent[i+1 : i+1+context_window_len]\n",
        "          for subsequent_token in subsequent_tokens:\n",
        "              key = tuple(sorted([subsequent_token, curr_word]))\n",
        "              self.total_occurrence_sum += 2\n",
        "              self.term_term_count[key] += 1\n",
        "              self.term_sums[curr_word] += 1\n",
        "              self.term_sums[subsequent_token] += 1\n",
        "\n",
        "    # Sort vocabulary\n",
        "    self.vocabulary = sorted(self.vocabulary)\n",
        "    print(f'Vocabulary count: {len(self.vocabulary)}')\n",
        "\n",
        "\n",
        "  def get_term_vector(self, term):\n",
        "    \"\"\"\n",
        "    Dynamically compute the vector representation of a term (can be multi-\n",
        "    worded). To get the multi-word term's vector a mean of all the vectors\n",
        "    of the words that make up the term is computed.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    corpus : term\n",
        "        The term that needs to be vectorized (can be multi-worded).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    numpy.ndarray of floats\n",
        "        A sparse vector representation of a term (can be muti-worded).\n",
        "    \"\"\"\n",
        "    term_vector_arr = []\n",
        "\n",
        "    # Split term into words in case it's a multi-word term\n",
        "    words = term.split(' ')\n",
        "    for word in words:\n",
        "      word_vector_arr = []\n",
        "      # Get occurrence sum for term of interest\n",
        "      term_sum = self.term_sums[word]\n",
        "\n",
        "      # Create term vector using term occurrence of all terms\n",
        "      for dict_term in self.vocabulary:\n",
        "        curr_term_sum = self.term_sums[dict_term]\n",
        "        curr_term_and_term_val = self.term_term_count[(word, dict_term)] + \\\n",
        "                                  self.term_term_count[(dict_term, word)]\n",
        "        ppmi_val = 0\n",
        "        if(curr_term_and_term_val == 0):\n",
        "          ppmi_val = 0\n",
        "        else:\n",
        "          # Ignore divide by 0 warning\n",
        "          with np.errstate(divide='ignore'):\n",
        "            ppmi_val = np.log((curr_term_and_term_val /\n",
        "                                self.total_occurrence_sum) / \\\n",
        "                              ((term_sum / self.total_occurrence_sum) * \\\n",
        "                                (curr_term_sum / self.total_occurrence_sum)))\n",
        "            if(ppmi_val < 0):\n",
        "              ppmi_val = 0\n",
        "        word_vector_arr.append(ppmi_val + self.smoother)\n",
        "      term_vector_arr.append(np.array(word_vector_arr))\n",
        "    term_vector_arr = np.array(term_vector_arr)\n",
        "\n",
        "    if(len(words) == 1):\n",
        "      term_vector_arr = term_vector_arr.reshape(term_vector_arr.shape[1],)\n",
        "    else:\n",
        "      term_vector_arr = np.mean(term_vector_arr, axis=0)\n",
        "\n",
        "    return np.array(word_vector_arr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEcHSrcbaYKM"
      },
      "source": [
        "Initialise PPMI class with corpus and context window"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjVCEb2EQ-Y6",
        "outputId": "e734d290-3691-4223-b7a6-0a21463f14ba"
      },
      "outputs": [],
      "source": [
        "start_time = time()\n",
        "\n",
        "ppmi_vectorizer = PPMIVectorizer()\n",
        "ppmi_vectorizer.count_terms_with_context(plot_synopses,\n",
        "                                         context_window_len=2)\n",
        "\n",
        "end_time = time()\n",
        "\n",
        "# Calculate elapsed time\n",
        "elapsed_time = str(timedelta(seconds=end_time - start_time))\n",
        "print(f\"Elapsed Time: {str(elapsed_time)[elapsed_time.find(':') + 1:]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FY2qn03KaJzE"
      },
      "source": [
        "### Compute Similarities Between Terms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJFVYLanaPHs"
      },
      "source": [
        "Compute similarities in validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JycaCZMrRN-s",
        "outputId": "0eede7e2-f571-41e9-fc9e-01c9aaad0015"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time taken to compute similarities for given data: 01:08.376483\n"
          ]
        }
      ],
      "source": [
        "predictions_ppmi_val = get_comparison_with_csv(Method.PPMI,\n",
        "                                               Data.VALIDATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2hl_QJKacY3"
      },
      "source": [
        "Compute similarities in test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNNPYHD6n_qG",
        "outputId": "2837404f-1c7c-446a-86ef-4a83e88836d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time taken to compute similarities for given data: 00:51.243617\n"
          ]
        }
      ],
      "source": [
        "predictions_ppmi_test = get_comparison_with_csv(Method.PPMI,\n",
        "                                                Data.TEST)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vWLpYV9lKrg"
      },
      "source": [
        "### Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-5fr69nlHUB"
      },
      "outputs": [],
      "source": [
        "predictions_ppmi_val.to_csv('ppmi_predictions_validation.csv',\n",
        "                            header=False,\n",
        "                            index=False)\n",
        "\n",
        "predictions_ppmi_test.to_csv('ppmi_predictions_test.csv',\n",
        "                             header=False,\n",
        "                             index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python3 scripts/term_similarity_eval_script.py ppmi_predictions_validation.csv data/Validation-dataset-terms.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcL1OEE4amO0"
      },
      "source": [
        "## METHOD B) Dense Static Representation\n",
        "### Word2Vec with Skip-gram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CqLMAMcldfg"
      },
      "source": [
        "### Impement and Initialise Word2Vec Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBVWOlXIseba"
      },
      "outputs": [],
      "source": [
        "class Word2vecVectorizer():\n",
        "  \"\"\"\n",
        "  A custom class for word2vec that allows to generate dense vector\n",
        "  representations for terms (including multi-word terms) trained on a corpus.\n",
        "\n",
        "  Attributes\n",
        "  ----------\n",
        "  model : gensim.Word2Vec\n",
        "      A Word2Vec instance that holds all vector representations.\n",
        "\n",
        "  Methods\n",
        "  -------\n",
        "  preprocess_corpus(corpus)\n",
        "      Preprocess a given corpus and return a list of tokenised sentences.\n",
        "  create_model(corpus, context_window_len, vector_size, sg, epochs)\n",
        "      Train word2vec on corpus with chosen hyperparameters.\n",
        "  get_term_vector(term)\n",
        "      Dynamically compute the vector representation of a term (can be multi-\n",
        "      worded).\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    self.model = None\n",
        "\n",
        "  def preprocess_corpus(self, corpus):\n",
        "    \"\"\"\n",
        "    Preprocess a given corpus and return a list of tokenised sentences.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    corpus : list of str\n",
        "        Series of strings containing all documents which need to be\n",
        "        preprocessed.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list of list of str\n",
        "        A list of lists of strings that are the tokenised words from the\n",
        "        corpus.\n",
        "    \"\"\"\n",
        "    word_tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    tokenized_sentences = []\n",
        "    for text in tqdm(corpus):\n",
        "        # Tokenize sentences\n",
        "        sents = sent_tokenize(text.lower())\n",
        "        for sent in sents:\n",
        "\n",
        "          # Tokenise each word\n",
        "          words = word_tokenizer.tokenize(sent)\n",
        "\n",
        "          tokenized_sentences.append(words)\n",
        "    return tokenized_sentences\n",
        "\n",
        "  def create_model(self,\n",
        "                   corpus,\n",
        "                   context_window_len=3,\n",
        "                   vector_size=200,\n",
        "                   sg=1,\n",
        "                   epochs=5):\n",
        "    \"\"\"\n",
        "    Train word2vec on corpus with chosen hyperparameters.\n",
        "    get_term_vector(term).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    corpus : pandas.Series of str\n",
        "        Series of strings containing all documents which will be used in\n",
        "        training the word2vec model.\n",
        "    context_window_len : int\n",
        "        The length of the context window which would indicate if a term falls\n",
        "        within another term's context.\n",
        "    vector_size : int\n",
        "        The dimensionality of the resulted vector representation of a term.\n",
        "    sg : int\n",
        "        Flag that indicated to either use CBOW or Skip-gram to train the\n",
        "        word2vec model. Use 0 for the former, 1 for the latter.\n",
        "    epochs : int\n",
        "        Count of epochs for which the model will be trained.\n",
        "    \"\"\"\n",
        "    # Train Word2Vec model\n",
        "\n",
        "    self.model = Word2Vec(sentences=self.preprocess_corpus(corpus),\n",
        "                          corpus_file=None,\n",
        "                          vector_size=vector_size,\n",
        "                          min_alpha=0.0001,\n",
        "                          alpha=0.025,\n",
        "                          window=context_window_len,\n",
        "                          min_count=5,\n",
        "                          max_vocab_size=None,\n",
        "                          sample=0.001,\n",
        "                          seed=1,\n",
        "                          workers=3,\n",
        "                          sg=sg,\n",
        "                          hs=0,\n",
        "                          negative=5,\n",
        "                          ns_exponent=0.75,\n",
        "                          cbow_mean=1,\n",
        "                          epochs=epochs,\n",
        "                          null_word=0,\n",
        "                          trim_rule=None,\n",
        "                          sorted_vocab=1,\n",
        "                          batch_words=10000,\n",
        "                          compute_loss=False,\n",
        "                          callbacks=(),\n",
        "                          comment=None,\n",
        "                          max_final_vocab=None,\n",
        "                          shrink_windows=True)\n",
        "\n",
        "  def get_term_vector(self, term):\n",
        "    \"\"\"\n",
        "    Retrieve the vector representation of a term (can be multi-\n",
        "    worded). To get the multi-word term's vector a mean of all the vectors\n",
        "    of the words that make up the term is computed.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    corpus : term\n",
        "        The term that needs to be vectorized (can be multi-worded).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    numpy.ndarray of floats\n",
        "        A dense vector representation of a term (can be muti-worded).\n",
        "    \"\"\"\n",
        "    term_vector_arr = []\n",
        "\n",
        "    # Split term into words in case it's a multi-word term\n",
        "    words = term.split(' ')\n",
        "    for word in words:\n",
        "      word_vector = []\n",
        "      if(word in self.model.wv):\n",
        "        word_vector = self.model.wv[word]\n",
        "      else:\n",
        "        word_vector = np.zeros(200)\n",
        "      term_vector_arr.append(np.array(word_vector))\n",
        "    term_vector_arr = np.array(term_vector_arr)\n",
        "\n",
        "    if(len(words) == 1):\n",
        "      term_vector_arr = term_vector_arr.reshape(term_vector_arr.shape[1],)\n",
        "    else:\n",
        "      term_vector_arr = np.mean(term_vector_arr, axis=0)\n",
        "\n",
        "    return np.array(term_vector_arr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0fwwP7Yaw9E"
      },
      "source": [
        "Initialise word2vec class with corpus, context window, vector dimensionality, implementation (CBOW or Skip-gram) and epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7thCAPjE1xLb",
        "outputId": "8db58b0d-6e35-4c1f-b745-ae37016ccda7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 8257/8257 [00:11<00:00, 734.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Elapsed Time: 02:28.520247\n"
          ]
        }
      ],
      "source": [
        "start_time = time()\n",
        "\n",
        "word2vec_vectorizer = Word2vecVectorizer()\n",
        "word2vec_vectorizer.create_model(plot_synopses,\n",
        "                                 context_window_len=3,\n",
        "                                 vector_size=200,\n",
        "                                 sg=1,\n",
        "                                 epochs=5)\n",
        "\n",
        "end_time = time()\n",
        "\n",
        "# Calculate elapsed time\n",
        "elapsed_time = str(timedelta(seconds=end_time - start_time))\n",
        "print(f\"Elapsed Time: {str(elapsed_time)[elapsed_time.find(':') + 1:]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2BL_rPVatDT"
      },
      "source": [
        "### Compute Similarities Between Terms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgjOTlv2ba-D"
      },
      "source": [
        "Compute similarities in validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ma0UeKQbRRAL",
        "outputId": "c93a3a02-7990-434a-9bdd-9e75b986675e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time taken to compute similarities for given data: 00:00.047226\n"
          ]
        }
      ],
      "source": [
        "predictions_word2vec_val = get_comparison_with_csv(Method.WORD2VEC,\n",
        "                                                   Data.VALIDATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeAaukCmbb-h"
      },
      "source": [
        "Compute similarities in test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edHfFVUVoaAr",
        "outputId": "72382e3e-cfea-48e1-8dac-5817078350a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time taken to compute similarities for given data: 00:00.032098\n"
          ]
        }
      ],
      "source": [
        "predictions_word2vec_test = get_comparison_with_csv(Method.WORD2VEC,\n",
        "                                                    Data.TEST)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uehHUmSylr1o"
      },
      "source": [
        "### Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Lb6S0UWlttT"
      },
      "outputs": [],
      "source": [
        "predictions_word2vec_val.to_csv('word2vec_predictions_validation.csv',\n",
        "                            header=False,\n",
        "                            index=False)\n",
        "\n",
        "predictions_word2vec_test.to_csv('word2vec_predictions_test.csv',\n",
        "                             header=False,\n",
        "                             index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python3 scripts/term_similarity_eval_script.py word2vec_predictions_validation.csv data/Validation-dataset-terms.csv"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
